#' @importFrom BiocParallel bplapply
.compute_mean_var <- function(x, block, design, subset.row, block.FUN, residual.FUN, BPPARAM, ...) {
    subset.row <- .subset_to_index(subset.row, x, byrow=TRUE)
    wout <- .worker_assign(length(subset.row), BPPARAM)
    by.core <- .split_vector_by_workers(subset.row, wout)
    by.core <- .split_matrix_by_workers(x, by.core)

    if (!is.null(block)) { 
        if (ncol(x)!=length(block)) {
            stop("length of 'block' should be the same as 'ncol(x)'")
        }

        # Checking residual d.f.
        by.block <- split(seq_len(ncol(x))-1L, block, drop=TRUE)
	} else {
        by.block <- list(seq_len(ncol(x))-1L)
	}

    resid.df <- lengths(by.block) - 1L
	if (all(resid.df<=0L)){ 
		stop("no residual d.f. in any level of 'block' for variance estimation")
	}

    if (is.null(design)) {
	    raw.stats <- bplapply(by.core, FUN=block.FUN, bygroup=by.block, ..., BPPARAM=BPPARAM)
        means <- do.call(rbind, lapply(raw.stats, FUN=function(x) t(x[[1]])))
        vars <- do.call(rbind, lapply(raw.stats, FUN=function(x) t(x[[2]])))
    } else {
        # Put linear modelling section here.
        means <- vars <- vector("list", length(by.block))

        for (i in seq_along(by.block)) {
            current <- by.block[[i]] + 1L
            curdesign <- design[current,,drop=FALSE]
            cur.core <- lapply(by.core, "[", , j=current, drop=FALSE)

            # Checking residual d.f.
            resid.df <- nrow(curdesign) - ncol(curdesign)
            if (resid.df <= 0L) {
                stop("no residual d.f. in 'design' for variance estimation")
            }
            QR <- .ranksafe_qr(curdesign)

            # Calculating the residual variance of the fitted linear model.
            raw.stats <- bplapply(cur.core, FUN=residual.FUN, qr=QR$qr, qraux=QR$qraux, ..., BPPARAM=BPPARAM)
            means[[i]] <- unlist(lapply(raw.stats, FUN="[[", i=1))
            vars[[i]] <- unlist(lapply(raw.stats, FUN="[[", i=2))
        }
    }

	dimnames(means) <- dimnames(vars) <- list(rownames(x)[subset.row], names(by.block))
    list(means=means, vars=vars, ncells=lengths(by.block))
}

#' @importFrom stats pnorm
#' @importFrom S4Vectors DataFrame metadata<-
.decompose_log_exprs <- function(x.means, x.vars, fit.means, fit.vars, ...) {
    collected <- vector("list", ncol(x.means))
    for (i in seq_along(collected)) {
        fm <- fit.means[,i]
        fv <- fit.vars[,i]
        fit <- fitTrendVar(fm, fv, ...)

        xm <- x.means[,i]
        output <- DataFrame(mean=xm, total=x.vars[,i], tech=fit$trend(xm))
        output$bio <- output$total - output$tech
        output$p.value <- pnorm(output$bio/output$tech, sd=fit$std.dev, lower.tail=FALSE)

        rownames(output) <- rownames(x.means)
        metadata(output) <- c(list(mean=fm, var=fv), fit)
        collected[[i]] <- output
    }
    collected
}

#' @importFrom stats pnorm
#' @importFrom S4Vectors DataFrame metadata<-
.decompose_cv2 <- function(x.means, x.vars, fit.means, fit.vars, ncells, ...) {
    collected <- vector("list", ncol(x.means))
    for (i in seq_along(collected)) {
        fm <- fit.means[,i]
        fcv2 <- fit.vars[,i]/fm^2
        fit <- fitTrendCV2(fm, fcv2, ncells[i], ...)

        xm <- x.means[,i]
        xcv2 <- x.vars[,i]/xm^2
        output <- DataFrame(mean=xm, total=xcv2, trend=fit$trend(xm))

        output$ratio <- output$total/output$trend
        output$p.value <- pnorm(output$ratio, mean=1, sd=fit$std.dev, lower.tail=FALSE)

        rownames(output) <- rownames(x.means)
        metadata(output) <- c(list(mean=fm, cv2=fcv2), fit)
        collected[[i]] <- output
    }
    collected
}

#' @importFrom SummarizedExperiment assayNames
#' @importFrom scater librarySizeFactors
#' @importFrom SingleCellExperiment sizeFactorNames
#' @importFrom BiocGenerics sizeFactors
.check_centered_SF <- function(x, assay.type, block=NULL)
# Checks if 'logcounts' was requested, and if it could have been computed from counts.
# If so, then it checks whether the size factors are centered across gene sets.
{
    if (assay.type=="logcounts" && "counts" %in% assayNames(x)) {
        if (is.null(block)) {
            by.block <- list(seq_len(ncol(x)))
        } else {
            by.block <- split(seq_len(ncol(x)), block)
        }

        FUN <- function(sfs) {
            means <- numeric(length(by.block))
            for (idx in seq_along(by.block)) {
                means[idx] <- mean(sfs[by.block[[idx]]])
            }
            return(means)
        }
        
        ref <- sizeFactors(x)
        if (is.null(ref)) {
            ref <- librarySizeFactors(x)
        }
        ref.means <- FUN(ref)

        is.okay <- TRUE
        for (sf in sizeFactorNames(x)) {
            sf.means <- FUN(sizeFactors(x, sf))
            if (!isTRUE(all.equal(ref.means, sf.means))) {
                is.okay <- FALSE
                break
            }
        }

        if (!is.okay) {
            if (is.null(block)) { 
                warning("size factors not centred, run 'normalize()' first")
            } else {
                warning("size factors not centred, run 'multiBlockNorm()' first")
            }
        }
    }
    return(NULL)
}

#' @importFrom BiocGenerics sizeFactors
#' @importFrom SingleCellExperiment spikeNames isSpike
.prepare_cv2_data <- function(x, spike.type) 
# Prepares data for calculation of CV2.
# In particular, extracting spike-ins and their size factors.    
{
    sf.cell <- sizeFactors(x)
    if (is.null(spike.type) || all(!is.na(spike.type))) { 
        if (is.null(spike.type)) { 
            # Get all spikes.
            spike.type <- spikeNames(x)            
        } else if (!all(spike.type %in% spikeNames(x))) { 
            stop(sprintf("spike-in set '%s' does not exist", spike.type[1]))
        }
        if (!length(spike.type)) { 
            stop("no spike-in sets specified from 'x'")
        }

        # Collecting the size factors for the requested spike-in sets.
        # Check that all spike-in factors are either NULL or identical.
        collected <- NULL
        is.spike <- logical(nrow(x))
        for (st in seq_along(spike.type)) {
            cur.type <- spike.type[st]
            is.spike <- is.spike | isSpike(x, type=cur.type)
            cur.sf <- sizeFactors(x, type=cur.type)
            if (st==1L) {
                collected <- cur.sf
            } else if (!isTRUE(all.equal(collected, cur.sf))) {
                stop("size factors differ between spike-in sets")
            }
        }

        # Otherwise, diverting to the cell-based size factors if all spike-in factors are NULL.
        if (!is.null(collected)) {
            sf.spike <- collected
        } else {
            warning("no spike-in size factors set, using cell-based factors")
            sf.spike <- sf.cell
        }

    } else {
        sf.spike <- sf.cell
        is.spike <- NA
    }

    list(is.spike=is.spike, sf.cell=sf.cell, sf.spike=sf.spike)
}

#' @importFrom stats p.adjust
#' @importFrom S4Vectors DataFrame
.combine_blocked_statistics <- function(collected, method, equiweight, ncells, geometric=FALSE,
    fields=c("mean", "total", "tech", "bio"), pval="p.value")
{
    if (length(collected)==1L) {
        return(collected[[1]])
    }

    # Combining statistics with optional weighting.
    if (equiweight) {
        weights <- rep(1, length(collected))
    } else {
        weights <- ncells
    }

    combined <- list()
    for (i in fields) {
        extracted <- lapply(collected, "[[", i=i)
        extracted <- mapply("*", extracted, weights, SIMPLIFY=FALSE, USE.NAMES=FALSE)

        if (geometric) {
            extracted <- lapply(extracted, log)
        }
        extracted <- mapply("*", extracted, weights, SIMPLIFY=FALSE)
        averaged <- Reduce("+", extracted)/sum(weights)
        if (geometric) {
            averaged <- exp(averaged)            
        }
        combined[[i]] <- averaged 
    }

    extracted <- lapply(collected, "[[", i=pval)
    combined$p.value <- do.call(combinePValues, c(extracted, list(method=method, weights=weights)))
    combined$FDR <- p.adjust(combined$p.value, method="BH")

    output <- DataFrame(combined)
    output$per.block <- do.call(DataFrame, lapply(collected, I))

    output
}

#' @importFrom BiocParallel SerialParam
#' @importFrom scater librarySizeFactors
.compute_var_stats_with_spikes <- function(x, spikes, size.factors=NULL, spike.size.factors=NULL, 
    subset.row=NULL, block=NULL, BPPARAM=SerialParam(), ...)
{
    if (is.null(size.factors)) {
        size.factors <- librarySizeFactors(x, subset_row=subset.row)        
    }
    stats.out <- .compute_mean_var(x, block=block, subset.row=subset.row, 
        BPPARAM=BPPARAM, sf=size.factors, ...)

    if (is.null(spike.size.factors)) {
        spike.size.factors <- librarySizeFactors(spikes) # no subset_row here, as that only applies to 'x'.
    }

    # Rescaling so that the mean spike.size.factors is the same as each size.factors in each block.
    if (is.null(block)) {
        spike.size.factors <- spike.size.factors / mean(spike.size.factors) * mean(size.factors)
    } else {
        by.block <- split(seq_along(block), block)
        for (i in by.block) {
            current <- spike.size.factors[i]
            spike.size.factors[i] <- current / mean(current) * mean(size.factors[i])
        }
    }

    spike.stats <- .compute_mean_var(spikes, block=block, subset.row=subset.row, 
        BPPARAM=BPPARAM, sf=spike.size.factors, ...)

    list(x=stats.out, spikes=spike.stats)
}

#' @importFrom stats density approx
.inverse_density_weights <- function(x, adjust=1) {
    out <- density(x, adjust=adjust, from=min(x), to=max(x))
    w <- 1/approx(out$x, out$y, xout=x)$y 
    w/mean(w)
}

#' @importFrom limma weighted.median
.correct_logged_expectation <- function(x, y, w, FUN) 
# Adjusting for any scale shift due to fitting to the log-values.
# The expectation of the log-values should be the log-expectation
# plus a factor that is dependent on the variance of the raw values
# divided by the squared mean, using a second-order Taylor approximation. 
# If we assume that the standard deviation of the variances is proportional
# to the mean variances with the same constant across all abundances,
# we should be able to correct the discrepancy with a single rescaling factor. 
{
    leftovers <- y/FUN(x)
    med <- weighted.median(leftovers, w, na.rm=TRUE)

    OUT <- function(x) { 
        output <- FUN(x) * med
        names(output) <- names(x)
        output
    }

    # We assume ratios are normally distributed around 1 with some standard deviation.
    std.dev <- unname(weighted.median(abs(leftovers/med - 1), w, na.rm=TRUE)) * 1.4826 
    list(trend=OUT, std.dev=std.dev)
}
